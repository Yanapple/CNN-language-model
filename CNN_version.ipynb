-{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Xpp_Dc7iQL9",
    "outputId": "9dcd5869-7a5c-443d-bf48-689701a52579",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !mkdir /root/.kaggle\n",
    "# import json\n",
    "# kaggle = {\"username\":\"yanapple\",\"key\":\"53c4e2d6e293bb5024208c4fdab61b4e\"}\n",
    "# with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
    "#     json.dump(kaggle, f)\n",
    "\n",
    "# !chmod 600 /root/.kaggle/kaggle.json\n",
    "# #Загружаем данные выбранного соревнования\n",
    "# !kaggle datasets download mhantor/russian-voice-dataset\n",
    "\n",
    "# ! unzip russian-voice-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "022NmSUqsmB0"
   },
   "source": [
    "Загрузка библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "id": "Pa03ZIMislKb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCk4pVvJspzw"
   },
   "source": [
    "<font size=\"5\">\n",
    "\n",
    "Загрузка данных.\n",
    "\n",
    "**main_dataset_dir** - общий путь к датасету. Для распаковки zip файла требуется явно указывать путь, без передач через переменные.\n",
    "\n",
    "**text_path** - путь\\название Excel файла, хранящего транскрипцию аудио.\n",
    "\n",
    "**audio_path** - путь\\название файла, хранящего wav файлы аудио."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhEL3t5WtsUs"
   },
   "source": [
    "Закомментировано до момента работоспособности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = 'Old_dataset/Disorder Voices/Disorder Voices'\n",
    "text_path = 'Old_dataset/Speeches.xlsx' #/JupiterNotebooks/CNN_svm/Old_dataset\n",
    "dir_name = audio_path + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "DAmiWGqxuElq"
   },
   "outputs": [],
   "source": [
    "# audio_path = '/content/Disorder Voices/Disorder Voices'\n",
    "# text_path = '/content/Speeches.xlsx'\n",
    "# dir_name = '/content/Disorder Voices/Disorder Voices/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BtYcNH1Qq3t5",
    "outputId": "6cda2f6d-7442-4e76-a884-8021b2155beb"
   },
   "outputs": [],
   "source": [
    "# main_dataset_dir = \"/content/Disorder_Russian_Speech.zip\"\n",
    "# text_path = \"\"\n",
    "# audio_path = \"\"\n",
    "\n",
    "# !unzip /content/Disorder_Russian_Speech.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "_LNepWgwr7GY"
   },
   "outputs": [],
   "source": [
    "amount_of_data_to_load = 100\n",
    "\n",
    "def data_load():\n",
    "    file = pd.read_excel(text_path) #/Speeches.xlsx\n",
    "    text_list = [sentence for sentence in file['Русская речь']][:amount_of_data_to_load]\n",
    "\n",
    "    files_in_dir = os.listdir(audio_path)\n",
    "\n",
    "    audio_list = []\n",
    "    i = 1\n",
    "\n",
    "    for e in range(1, amount_of_data_to_load + 1):\n",
    "        file_name = f'{e}.wav'\n",
    "        sampl = librosa.load(dir_name + file_name, sr=8000)[0]\n",
    "        sampl = sampl[np.newaxis, :]\n",
    "        audio_list.append(torch.Tensor(sampl))\n",
    "\n",
    "    return audio_list, text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ly-1DHZowvX7"
   },
   "source": [
    "<font size=\"5\">\n",
    "\n",
    "С помощью **char_map** функция **remove_characters** позволяет отфильтровать текстовые данные (данным образом из списка, полученного из Excel). Функция оставит лишь релевантные знаки и буквы, исключив ошибки вроде случайных, не несущих нам важности знаков: :№;@().\n",
    "\n",
    "**char_map** - словарь для замены векторного значения буквы и обратно. Так же использовано для фильтра."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "mRI_tygIwpjc"
   },
   "outputs": [],
   "source": [
    "char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n",
    "            \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n",
    "            \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n",
    "            \"я\": 31, \"х\": 32, \" \": 33, \"[UNK]\": 34, \"[PAD]\": 35}\n",
    "\n",
    "def remove_characters(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join(filter(lambda x: x in char_map, sentence))\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "085nMN_y00rX"
   },
   "source": [
    "<font size=\"5\">\n",
    "\n",
    "**split_train_test** разделяет датасет на тренировочный и обучающие датасеты, требующиеся для тренировки модели и корректировки ее поведения с помощью обучающего датасета.\n",
    "По дефолту разделяет датасет на train\\test в пропорции 9 к 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "MVpAWNDR0amN"
   },
   "outputs": [],
   "source": [
    "def split_train_test(X, y):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3ApQmbL1rMf"
   },
   "source": [
    "<font size=\"5\">\n",
    "\n",
    "Класс **AudioDataset** принимает только класс **Dataset**.\n",
    "Позволяет оформить датасет библиотеки Pytorch, который требуется для обучения модели. Инкапсулирующий источник данных.\n",
    "\n",
    "\n",
    "\n",
    "**train_dataset** - датасет для обучения. Размера train_dataset.size()\n",
    "\n",
    "**test_dataset** - датасет для тестирования. Размера test_dataset.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "1VrN1D2jt3PT"
   },
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_list, text_list):\n",
    "        self.audio_list = audio_list\n",
    "        self.text_list = text_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio = self.audio_list[index]\n",
    "        text = self.text_list[index]\n",
    "        return audio, text\n",
    "\n",
    "X, y = data_load()\n",
    "y = list(map(remove_characters, y))\n",
    "X_train, X_test, y_train, y_test = split_train_test(X, y)\n",
    "train_dataset = AudioDataset(X_train, y_train)\n",
    "test_dataset = AudioDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3ApQmbL1rMf"
   },
   "source": [
    "<font size=\"5\">\n",
    "\n",
    "\n",
    "**train_dataset** - датасет для обучения. Размера train_dataset.size()\n",
    "\n",
    "**test_dataset** - датасет для тестирования. Размера test_dataset.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransform:\n",
    "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
    "    def __init__(self):\n",
    "        self.char_map = {\"а\": 0, \"б\": 1, \"в\": 2, \"г\": 3, \"д\": 4, \"е\": 5, \"ё\": 6, \"ж\": 7, \"з\": 8, \"и\": 9, \"й\": 10,\n",
    "                  \"к\": 11, \"л\": 12, \"м\": 13, \"н\": 14, \"о\": 15, \"п\": 16, \"р\": 17, \"с\": 18, \"т\": 19, \"у\": 20,\n",
    "                  \"ф\": 21, \"ч\": 22, \"ц\": 23, \"ш\": 24, \"щ\": 25, \"ъ\": 26, \"ы\": 27, \"ь\": 28, \"э\": 29, \"ю\": 30,\n",
    "                  \"я\": 31, \"х\": 32, \" \": 33}\n",
    "\n",
    "        self.index_map = {}\n",
    "        for key, value in self.char_map.items():\n",
    "            self.index_map[value] = key\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
    "        int_sequence = []\n",
    "        for c in text:\n",
    "            if c != '':\n",
    "                ch = self.char_map[c]\n",
    "            int_sequence.append(ch)\n",
    "        return int_sequence\n",
    "\n",
    "    def int_to_text(self, labels):\n",
    "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
    "        string = []\n",
    "        for i in labels:\n",
    "            string.append(self.index_map[i])\n",
    "        return ''.join(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio_transforms = nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
    ")\n",
    "\n",
    "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
    "\n",
    "text_transform = TextTransform()\n",
    "\n",
    "def data_processing(data, data_type=\"train\"):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "    for (waveform, utterance) in data:\n",
    "        if data_type == 'train':\n",
    "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        elif data_type == 'valid':\n",
    "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        else:\n",
    "            raise Exception('data_type should be train or valid')\n",
    "        spectrograms.append(spec)\n",
    "        label = torch.Tensor(text_transform.text_to_int(utterance))\n",
    "        labels.append(label)\n",
    "        input_lengths.append(spec.shape[0]//4)\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    spectrograms1 = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return spectrograms1, labels, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">\n",
    "\n",
    "Методы выше data_processing и TextTransform преобразуют аудио к виду MFCC типа Tensor [[float, float],[]...], соответствующих значениям частот, разбитых на \"окна\", и текст к виду  [int, int]. соответствующих числовым значениям букв (включая пробел, неизвестные символы и padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_using = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
    "                                **kwargs)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
    "                                **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_wer(wer_scores, combined_ref_len):\n",
    "    return float(sum(wer_scores)) / float(combined_ref_len)\n",
    "\n",
    "\n",
    "def _levenshtein_distance(ref, hyp):\n",
    "    \"\"\"Levenshtein distance is a string metric for measuring the difference\n",
    "    between two sequences. Informally, the levenshtein disctance is defined as\n",
    "    the minimum number of single-character edits (substitutions, insertions or\n",
    "    deletions) required to change one word into the other. We can naturally\n",
    "    extend the edits to word level when calculate levenshtein disctance for\n",
    "    two sentences.\n",
    "    \"\"\"\n",
    "    m = len(ref)\n",
    "    n = len(hyp)\n",
    "\n",
    "    # special case\n",
    "    if ref == hyp:\n",
    "        return 0\n",
    "    if m == 0:\n",
    "        return n\n",
    "    if n == 0:\n",
    "        return m\n",
    "\n",
    "    if m < n:\n",
    "        ref, hyp = hyp, ref\n",
    "        m, n = n, m\n",
    "\n",
    "    # use O(min(m, n)) space\n",
    "    distance = np.zeros((2, n + 1), dtype=np.int32)\n",
    "\n",
    "    # initialize distance matrix\n",
    "    for j in range(0,n + 1):\n",
    "        distance[0][j] = j\n",
    "\n",
    "    # calculate levenshtein distance\n",
    "    for i in range(1, m + 1):\n",
    "        prev_row_idx = (i - 1) % 2\n",
    "        cur_row_idx = i % 2\n",
    "        distance[cur_row_idx][0] = i\n",
    "        for j in range(1, n + 1):\n",
    "            if ref[i - 1] == hyp[j - 1]:\n",
    "                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n",
    "            else:\n",
    "                s_num = distance[prev_row_idx][j - 1] + 1\n",
    "                i_num = distance[cur_row_idx][j - 1] + 1\n",
    "                d_num = distance[prev_row_idx][j] + 1\n",
    "                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n",
    "\n",
    "    return distance[m % 2][n]\n",
    "\n",
    "\n",
    "def word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
    "    \"\"\"Compute the levenshtein distance between reference sequence and\n",
    "    hypothesis sequence in word-level.\n",
    "    :param reference: The reference sentence.\n",
    "    :type reference: basestring\n",
    "    :param hypothesis: The hypothesis sentence.\n",
    "    :type hypothesis: basestring\n",
    "    :param ignore_case: Whether case-sensitive or not.\n",
    "    :type ignore_case: bool\n",
    "    :param delimiter: Delimiter of input sentences.\n",
    "    :type delimiter: char\n",
    "    :return: Levenshtein distance and word number of reference sentence.\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    if ignore_case == True:\n",
    "        reference = reference.lower()\n",
    "        hypothesis = hypothesis.lower()\n",
    "\n",
    "    ref_words = reference.split(delimiter)\n",
    "    hyp_words = hypothesis.split(delimiter)\n",
    "\n",
    "    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n",
    "    return float(edit_distance), len(ref_words)\n",
    "\n",
    "\n",
    "def char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n",
    "    \"\"\"Compute the levenshtein distance between reference sequence and\n",
    "    hypothesis sequence in char-level.\n",
    "    :param reference: The reference sentence.\n",
    "    :type reference: basestring\n",
    "    :param hypothesis: The hypothesis sentence.\n",
    "    :type hypothesis: basestring\n",
    "    :param ignore_case: Whether case-sensitive or not.\n",
    "    :type ignore_case: bool\n",
    "    :param remove_space: Whether remove internal space characters\n",
    "    :type remove_space: bool\n",
    "    :return: Levenshtein distance and length of reference sentence.\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    if ignore_case == True:\n",
    "        reference = reference.lower()\n",
    "        hypothesis = hypothesis.lower()\n",
    "\n",
    "    join_char = ' '\n",
    "    if remove_space == True:\n",
    "        join_char = ''\n",
    "\n",
    "    reference = join_char.join(filter(None, reference.split(' ')))\n",
    "    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n",
    "\n",
    "    edit_distance = _levenshtein_distance(reference, hypothesis)\n",
    "    return float(edit_distance), len(reference)\n",
    "\n",
    "\n",
    "def wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
    "    \"\"\"Calculate word error rate (WER). WER compares reference text and\n",
    "    hypothesis text in word-level. WER is defined as:\n",
    "    .. math::\n",
    "        WER = (Sw + Dw + Iw) / Nw\n",
    "    where\n",
    "    .. code-block:: text\n",
    "        Sw is the number of words subsituted,\n",
    "        Dw is the number of words deleted,\n",
    "        Iw is the number of words inserted,\n",
    "        Nw is the number of words in the reference\n",
    "    We can use levenshtein distance to calculate WER. Please draw an attention\n",
    "    that empty items will be removed when splitting sentences by delimiter.\n",
    "    :param reference: The reference sentence.\n",
    "    :type reference: basestring\n",
    "    :param hypothesis: The hypothesis sentence.\n",
    "    :type hypothesis: basestring\n",
    "    :param ignore_case: Whether case-sensitive or not.\n",
    "    :type ignore_case: bool\n",
    "    :param delimiter: Delimiter of input sentences.\n",
    "    :type delimiter: char\n",
    "    :return: Word error rate.\n",
    "    :rtype: float\n",
    "    :raises ValueError: If word number of reference is zero.\n",
    "    \"\"\"\n",
    "    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n",
    "                                         delimiter)\n",
    "\n",
    "    if ref_len == 0:\n",
    "        raise ValueError(\"Reference's word number should be greater than 0.\")\n",
    "\n",
    "    wer = float(edit_distance) / ref_len\n",
    "    return wer\n",
    "\n",
    "\n",
    "def cer(reference, hypothesis, ignore_case=False, remove_space=False):\n",
    "    \"\"\"Calculate charactor error rate (CER). CER compares reference text and\n",
    "    hypothesis text in char-level. CER is defined as:\n",
    "    .. math::\n",
    "        CER = (Sc + Dc + Ic) / Nc\n",
    "    where\n",
    "    .. code-block:: text\n",
    "        Sc is the number of characters substituted,\n",
    "        Dc is the number of characters deleted,\n",
    "        Ic is the number of characters inserted\n",
    "        Nc is the number of characters in the reference\n",
    "    We can use levenshtein distance to calculate CER. Chinese input should be\n",
    "    encoded to unicode. Please draw an attention that the leading and tailing\n",
    "    space characters will be truncated and multiple consecutive space\n",
    "    characters in a sentence will be replaced by one space character.\n",
    "    :param reference: The reference sentence.\n",
    "    :type reference: basestring\n",
    "    :param hypothesis: The hypothesis sentence.\n",
    "    :type hypothesis: basestring\n",
    "    :param ignore_case: Whether case-sensitive or not.\n",
    "    :type ignore_case: bool\n",
    "    :param remove_space: Whether remove internal space characters\n",
    "    :type remove_space: bool\n",
    "    :return: Character error rate.\n",
    "    :rtype: float\n",
    "    :raises ValueError: If the reference length is zero.\n",
    "    \"\"\"\n",
    "    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n",
    "                                         remove_space)\n",
    "\n",
    "    if ref_len == 0:\n",
    "        raise ValueError(\"Length of reference should be greater than 0.\")\n",
    "\n",
    "    cer = float(edit_distance) / ref_len\n",
    "    return cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechRecognitionModel1(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SpeechRecognitionModel1, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),\n",
    "            nn.Conv2d(64, 128, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),\n",
    "        )\n",
    "\n",
    "        self.rnn = nn.GRU(input_size=2048,\n",
    "                    hidden_size=256,\n",
    "                    num_layers=1,\n",
    "                    batch_first=True,\n",
    "                    bidirectional=True)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.permute(0, 3, 1, 2) #перестановка измерений многомерных массивов\n",
    "        x = x.view(x.size(0), x.size(1), -1) #reshapes the tensor without copying memory, количество элементов постоянно\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.fc(x) #(10,53,34) итоговый размер\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterMeter(object):\n",
    "    \"\"\"keeps track of total iterations\"\"\"\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.val += 1\n",
    "\n",
    "    def get(self):\n",
    "        return self.val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter):\n",
    "    model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "    for batch_idx, _data in enumerate(train_loader):\n",
    "        spectrograms, labels, input_lengths, label_lengths = _data\n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(spectrograms)  # (batch, time, n_class)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        iter_meter.step()\n",
    "        if batch_idx % 10 == 0 or batch_idx == data_len:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(spectrograms), data_len,\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, criterion, epoch, iter_meter):\n",
    "    print('\\nevaluating...')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_cer, test_wer = [], []\n",
    "    with torch.no_grad():\n",
    "        for i, _data in enumerate(test_loader):\n",
    "            spectrograms, labels, input_lengths, label_lengths = _data\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "            for j in range(len(decoded_preds)):\n",
    "                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
    "                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "\n",
    "\n",
    "    avg_cer = sum(test_cer)/len(test_cer)\n",
    "    avg_wer = sum(test_wer)/len(test_wer)\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    decodes = []\n",
    "    targets = []\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "        decode = []\n",
    "        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
    "        for j, index in enumerate(args):\n",
    "            if index != blank_label:\n",
    "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
    "                    continue\n",
    "                decode.append(index.item())\n",
    "        decodes.append(text_transform.int_to_text(decode))\n",
    "    return decodes, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(learning_rate=5e-4, batch_size=20, epochs=10):\n",
    "\n",
    "    hparams = {\n",
    "        \"n_cnn_layers\": 2,\n",
    "        \"n_rnn_layers\": 2,\n",
    "        \"rnn_dim\": 256,\n",
    "        \"n_class\": 34,\n",
    "        \"n_feats\": 128,\n",
    "        \"stride\":2,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs\n",
    "    }\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    torch.manual_seed(7)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    train_dataset = AudioDataset(X_train, y_train)\n",
    "    test_dataset = AudioDataset(X_test, y_test)\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "    model = SpeechRecognitionModel1(34).to(device)\n",
    "\n",
    "    print(model)\n",
    "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "    criterion = nn.CTCLoss(blank=28).to(device)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'],\n",
    "                                            steps_per_epoch=int(len(train_loader)),\n",
    "                                            epochs=hparams['epochs'],\n",
    "                                            anneal_strategy='linear')\n",
    "\n",
    "    iter_meter = IterMeter()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n",
    "        test(model, device, test_loader, criterion, epoch, iter_meter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpeechRecognitionModel1(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): GELU(approximate='none')\n",
      "    (7): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): GELU(approximate='none')\n",
      "    (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): GELU(approximate='none')\n",
      "    (12): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (rnn): GRU(2048, 256, batch_first=True, bidirectional=True)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=128, out_features=34, bias=True)\n",
      "  )\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n",
      "Num Model Parameters 3797794\n",
      "Train Epoch: 1 [0/90 (0%)]\tLoss: 6.043152\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 6.0573, Average CER: 1.015000 Average WER: 1.0000\n",
      "\n",
      "Train Epoch: 2 [0/90 (0%)]\tLoss: 5.990074\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 5.8364, Average CER: 0.951780 Average WER: 1.0000\n",
      "\n",
      "Train Epoch: 3 [0/90 (0%)]\tLoss: 5.904147\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 5.5137, Average CER: 0.923131 Average WER: 1.0000\n",
      "\n",
      "Train Epoch: 4 [0/90 (0%)]\tLoss: 5.708163\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 5.0129, Average CER: 0.978131 Average WER: 1.0000\n",
      "\n",
      "Train Epoch: 5 [0/90 (0%)]\tLoss: 5.551080\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 4.2129, Average CER: 1.035492 Average WER: 1.0000\n",
      "\n",
      "Train Epoch: 6 [0/90 (0%)]\tLoss: 5.007156\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 3.7968, Average CER: 1.056326 Average WER: 1.0000\n",
      "\n",
      "Train Epoch: 7 [0/90 (0%)]\tLoss: 4.265249\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 3.7458, Average CER: 1.016427 Average WER: 1.0000\n",
      "\n",
      "Train Epoch: 8 [0/90 (0%)]\tLoss: 3.864696\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 3.7339, Average CER: 0.958510 Average WER: 1.0000\n",
      "\n",
      "Train Epoch: 9 [0/90 (0%)]\tLoss: 4.097936\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 3.7301, Average CER: 0.967399 Average WER: 1.0000\n",
      "\n",
      "Train Epoch: 10 [0/90 (0%)]\tLoss: 4.085294\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 3.7501, Average CER: 1.019066 Average WER: 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Необработанный формат ячейки",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
